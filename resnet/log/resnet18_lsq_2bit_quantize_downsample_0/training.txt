10/16 03:00:56 PM args = Namespace(batch_size=256, data='/home/sliuau/ternary_vit/dataset/imagenet-1k/imagenet', epochs=128, label_smooth=0.1, learning_rate=0.00125, load_pretrained_weight_student=None, momentum=0.9, n_bit=2, quantize_downsample='0', save='./models', student='resnet18_lsq', teacher='resnet101', weight_decay=0.0, workers=40)
10/16 03:00:58 PM student:
10/16 03:00:58 PM ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=64)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=64)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=64)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=64)
      (bias32): LearnableBias()
    )
  )
  (layer2): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=128)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=128)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=128)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=128)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=128)
      (bias32): LearnableBias()
    )
  )
  (layer3): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=128)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=256)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=256)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=256)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=256)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=256)
      (bias32): LearnableBias()
    )
  )
  (layer4): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=256)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=512)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=512)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=512)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=512)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=512)
      (bias32): LearnableBias()
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
weight_param: conv1.weight
weight_param: layer1.0.conv1.weight
weight_param: layer1.0.conv2.weight
weight_param: layer1.1.conv1.weight
weight_param: layer1.1.conv2.weight
weight_param: layer2.0.conv1.weight
weight_param: layer2.0.conv2.weight
weight_param: layer2.0.downsample.0.weight
weight_param: layer2.1.conv1.weight
weight_param: layer2.1.conv2.weight
weight_param: layer3.0.conv1.weight
weight_param: layer3.0.conv2.weight
weight_param: layer3.0.downsample.0.weight
weight_param: layer3.1.conv1.weight
weight_param: layer3.1.conv2.weight
weight_param: layer4.0.conv1.weight
weight_param: layer4.0.conv2.weight
weight_param: layer4.0.downsample.0.weight
weight_param: layer4.1.conv1.weight
weight_param: layer4.1.conv2.weight
10/16 03:04:33 PM args = Namespace(batch_size=256, data='/home/sliuau/ternary_vit/dataset/imagenet-1k/imagenet', epochs=128, label_smooth=0.1, learning_rate=0.00125, load_pretrained_weight_student=None, momentum=0.9, n_bit=2, quantize_downsample='0', save='./models', student='resnet18_lsq', teacher='resnet101', weight_decay=0.0, workers=40)
10/16 03:04:35 PM student:
10/16 03:04:35 PM ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=64)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=64)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=64)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=64)
      (bias32): LearnableBias()
    )
  )
  (layer2): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=128)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=128)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=128)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=128)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=128)
      (bias32): LearnableBias()
    )
  )
  (layer3): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=128)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=256)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=256)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=256)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=256)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=256)
      (bias32): LearnableBias()
    )
  )
  (layer4): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=256)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=512)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=512)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=512)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=512)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=512)
      (bias32): LearnableBias()
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
weight_param: conv1.weight
weight_param: layer1.0.conv1.weight
weight_param: layer1.0.conv2.weight
weight_param: layer1.1.conv1.weight
weight_param: layer1.1.conv2.weight
weight_param: layer2.0.conv1.weight
weight_param: layer2.0.conv2.weight
weight_param: layer2.0.downsample.0.weight
weight_param: layer2.1.conv1.weight
weight_param: layer2.1.conv2.weight
weight_param: layer3.0.conv1.weight
weight_param: layer3.0.conv2.weight
weight_param: layer3.0.downsample.0.weight
weight_param: layer3.1.conv1.weight
weight_param: layer3.1.conv2.weight
weight_param: layer4.0.conv1.weight
weight_param: layer4.0.conv2.weight
weight_param: layer4.0.downsample.0.weight
weight_param: layer4.1.conv1.weight
weight_param: layer4.1.conv2.weight
checkpoint_tar:  ./models/resnet18-f37072fd.pth
True
10/16 03:05:32 PM args = Namespace(batch_size=256, data='/home/sliuau/ternary_vit/dataset/imagenet-1k/imagenet', epochs=128, label_smooth=0.1, learning_rate=0.00125, load_pretrained_weight_student=None, momentum=0.9, n_bit=2, quantize_downsample='0', save='./models', student='resnet18_lsq', teacher='resnet101', weight_decay=0.0, workers=40)
10/16 03:05:35 PM student:
10/16 03:05:35 PM ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=64)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=64)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=64)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=64)
      (bias32): LearnableBias()
    )
  )
  (layer2): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=128)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=128)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=128)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=128)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=128)
      (bias32): LearnableBias()
    )
  )
  (layer3): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=128)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=256)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=256)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=256)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=256)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=256)
      (bias32): LearnableBias()
    )
  )
  (layer4): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=256)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=512)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=512)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=512)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=512)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=512)
      (bias32): LearnableBias()
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
weight_param: conv1.weight
weight_param: layer1.0.conv1.weight
weight_param: layer1.0.conv2.weight
weight_param: layer1.1.conv1.weight
weight_param: layer1.1.conv2.weight
weight_param: layer2.0.conv1.weight
weight_param: layer2.0.conv2.weight
weight_param: layer2.0.downsample.0.weight
weight_param: layer2.1.conv1.weight
weight_param: layer2.1.conv2.weight
weight_param: layer3.0.conv1.weight
weight_param: layer3.0.conv2.weight
weight_param: layer3.0.downsample.0.weight
weight_param: layer3.1.conv1.weight
weight_param: layer3.1.conv2.weight
weight_param: layer4.0.conv1.weight
weight_param: layer4.0.conv2.weight
weight_param: layer4.0.downsample.0.weight
weight_param: layer4.1.conv1.weight
weight_param: layer4.1.conv2.weight
checkpoint_tar:  ./models/resnet18-f37072fd.pth
True
10/16 03:07:22 PM args = Namespace(batch_size=256, data='/home/sliuau/ternary_vit/dataset/imagenet-1k/imagenet', epochs=128, label_smooth=0.1, learning_rate=0.00125, load_pretrained_weight_student=None, momentum=0.9, n_bit=2, quantize_downsample='0', save='./models', student='resnet18_lsq', teacher='resnet101', weight_decay=0.0, workers=40)
10/16 03:07:24 PM student:
10/16 03:07:24 PM ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=64)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=64)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=64)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=64)
      (bias32): LearnableBias()
    )
  )
  (layer2): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=128)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=128)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=128)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=128)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=128)
      (bias32): LearnableBias()
    )
  )
  (layer3): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=128)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=256)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=256)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=256)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=256)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=256)
      (bias32): LearnableBias()
    )
  )
  (layer4): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=256)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=512)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=512)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=512)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=512)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=512)
      (bias32): LearnableBias()
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
weight_param: conv1.weight
weight_param: layer1.0.conv1.weight
weight_param: layer1.0.conv2.weight
weight_param: layer1.1.conv1.weight
weight_param: layer1.1.conv2.weight
weight_param: layer2.0.conv1.weight
weight_param: layer2.0.conv2.weight
weight_param: layer2.0.downsample.0.weight
weight_param: layer2.1.conv1.weight
weight_param: layer2.1.conv2.weight
weight_param: layer3.0.conv1.weight
weight_param: layer3.0.conv2.weight
weight_param: layer3.0.downsample.0.weight
weight_param: layer3.1.conv1.weight
weight_param: layer3.1.conv2.weight
weight_param: layer4.0.conv1.weight
weight_param: layer4.0.conv2.weight
weight_param: layer4.0.downsample.0.weight
weight_param: layer4.1.conv1.weight
weight_param: layer4.1.conv2.weight
checkpoint_tar:  ./models/resnet18-f37072fd.pth
True
Test: [  0/196]	Time 17.751 (17.751)	Loss 8.0371e+00 (8.0371e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [  1/196]	Time  0.097 ( 8.924)	Loss 7.0530e+00 (7.5451e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [  2/196]	Time  0.073 ( 5.974)	Loss 6.3356e+00 (7.1419e+00)	Acc@1   0.39 (  0.13)	Acc@5   5.86 (  1.95)
Test: [  3/196]	Time  0.073 ( 4.499)	Loss 6.1850e+00 (6.9027e+00)	Acc@1   0.00 (  0.10)	Acc@5   0.00 (  1.46)
Test: [  4/196]	Time  0.074 ( 3.614)	Loss 6.6081e+00 (6.8437e+00)	Acc@1   0.39 (  0.16)	Acc@5   3.12 (  1.80)
Test: [  5/196]	Time  0.074 ( 3.024)	Loss 7.6050e+00 (6.9706e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  1.50)
Test: [  6/196]	Time  0.074 ( 2.602)	Loss 7.7467e+00 (7.0815e+00)	Acc@1   0.00 (  0.11)	Acc@5   0.00 (  1.28)
Test: [  7/196]	Time  0.074 ( 2.286)	Loss 6.6300e+00 (7.0251e+00)	Acc@1   0.00 (  0.10)	Acc@5   0.00 (  1.12)
Test: [  8/196]	Time  0.074 ( 2.040)	Loss 6.9915e+00 (7.0213e+00)	Acc@1   0.00 (  0.09)	Acc@5   0.00 (  1.00)
Test: [  9/196]	Time  0.077 ( 1.844)	Loss 6.9523e+00 (7.0144e+00)	Acc@1   0.00 (  0.08)	Acc@5   0.00 (  0.90)
Test: [ 10/196]	Time  0.074 ( 1.683)	Loss 7.2323e+00 (7.0342e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.82)
Test: [ 11/196]	Time  0.074 ( 1.549)	Loss 6.6184e+00 (6.9996e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.75)
Test: [ 12/196]	Time  0.070 ( 1.435)	Loss 6.9263e+00 (6.9939e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.69)
Test: [ 13/196]	Time  0.074 ( 1.338)	Loss 7.9268e+00 (7.0606e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.64)
Test: [ 14/196]	Time  0.070 ( 1.254)	Loss 7.4828e+00 (7.0887e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.60)
Test: [ 15/196]	Time  0.069 ( 1.179)	Loss 6.8707e+00 (7.0751e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.39 (  0.59)
Test: [ 16/196]	Time  0.068 ( 1.114)	Loss 5.3803e+00 (6.9754e+00)	Acc@1   2.73 (  0.21)	Acc@5  14.06 (  1.38)
Test: [ 17/196]	Time  0.069 ( 1.056)	Loss 6.3919e+00 (6.9430e+00)	Acc@1   0.00 (  0.20)	Acc@5   0.00 (  1.30)
Test: [ 18/196]	Time  0.070 ( 1.004)	Loss 6.4946e+00 (6.9194e+00)	Acc@1   0.00 (  0.19)	Acc@5   2.73 (  1.38)
Test: [ 19/196]	Time  0.069 ( 0.957)	Loss 7.6918e+00 (6.9580e+00)	Acc@1   0.00 (  0.18)	Acc@5   0.00 (  1.31)
Test: [ 20/196]	Time  0.070 ( 0.915)	Loss 7.4955e+00 (6.9836e+00)	Acc@1   0.00 (  0.17)	Acc@5   0.00 (  1.25)
Test: [ 21/196]	Time  0.070 ( 0.877)	Loss 7.9013e+00 (7.0253e+00)	Acc@1   0.00 (  0.16)	Acc@5   0.00 (  1.19)
Test: [ 22/196]	Time  0.069 ( 0.842)	Loss 7.0533e+00 (7.0265e+00)	Acc@1   0.00 (  0.15)	Acc@5   0.00 (  1.14)
Test: [ 23/196]	Time  0.071 ( 0.809)	Loss 8.1147e+00 (7.0719e+00)	Acc@1   0.00 (  0.15)	Acc@5   0.00 (  1.09)
Test: [ 24/196]	Time  0.072 ( 0.780)	Loss 7.0255e+00 (7.0700e+00)	Acc@1   0.00 (  0.14)	Acc@5   0.00 (  1.05)
Test: [ 25/196]	Time  0.074 ( 0.753)	Loss 7.2438e+00 (7.0767e+00)	Acc@1   0.00 (  0.14)	Acc@5   0.00 (  1.01)
Test: [ 26/196]	Time  0.071 ( 0.728)	Loss 6.9050e+00 (7.0703e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.97)
Test: [ 27/196]	Time  0.075 ( 0.704)	Loss 8.1273e+00 (7.1081e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.93)
Test: [ 28/196]	Time  0.165 ( 0.686)	Loss 8.1482e+00 (7.1440e+00)	Acc@1   0.00 (  0.12)	Acc@5   0.00 (  0.90)
Test: [ 29/196]	Time  0.070 ( 0.665)	Loss 7.3478e+00 (7.1508e+00)	Acc@1   0.00 (  0.12)	Acc@5   0.00 (  0.87)
Test: [ 30/196]	Time  0.070 ( 0.646)	Loss 8.0130e+00 (7.1786e+00)	Acc@1   0.00 (  0.11)	Acc@5   0.00 (  0.84)
Test: [ 31/196]	Time  0.070 ( 0.628)	Loss 7.2151e+00 (7.1797e+00)	Acc@1   0.00 (  0.11)	Acc@5   0.00 (  0.82)
Test: [ 32/196]	Time  0.069 ( 0.611)	Loss 7.9666e+00 (7.2036e+00)	Acc@1   0.00 (  0.11)	Acc@5   0.00 (  0.79)
Test: [ 33/196]	Time  0.070 ( 0.595)	Loss 7.2087e+00 (7.2037e+00)	Acc@1   0.00 (  0.10)	Acc@5   0.00 (  0.77)
Test: [ 34/196]	Time  0.071 ( 0.580)	Loss 7.3362e+00 (7.2075e+00)	Acc@1   0.00 (  0.10)	Acc@5   0.00 (  0.75)
Test: [ 35/196]	Time  0.070 ( 0.566)	Loss 7.3715e+00 (7.2120e+00)	Acc@1   0.00 (  0.10)	Acc@5   0.00 (  0.73)
Test: [ 36/196]	Time  0.070 ( 0.552)	Loss 7.1658e+00 (7.2108e+00)	Acc@1   0.00 (  0.10)	Acc@5   0.00 (  0.71)
Test: [ 37/196]	Time  0.070 ( 0.540)	Loss 7.4505e+00 (7.2171e+00)	Acc@1   0.00 (  0.09)	Acc@5   0.00 (  0.69)
Test: [ 38/196]	Time  0.070 ( 0.528)	Loss 7.2754e+00 (7.2186e+00)	Acc@1   0.00 (  0.09)	Acc@5   0.00 (  0.67)
Test: [ 39/196]	Time  0.072 ( 0.516)	Loss 7.1205e+00 (7.2161e+00)	Acc@1   0.00 (  0.09)	Acc@5   0.00 (  0.65)
Test: [ 40/196]	Time  0.072 ( 0.506)	Loss 7.4510e+00 (7.2219e+00)	Acc@1   0.00 (  0.09)	Acc@5   0.00 (  0.64)
Test: [ 41/196]	Time  0.069 ( 0.495)	Loss 7.3939e+00 (7.2260e+00)	Acc@1   0.00 (  0.08)	Acc@5   0.00 (  0.62)
Test: [ 42/196]	Time  0.070 ( 0.485)	Loss 7.2122e+00 (7.2256e+00)	Acc@1   0.00 (  0.08)	Acc@5   0.00 (  0.61)
Test: [ 43/196]	Time  0.069 ( 0.476)	Loss 7.4224e+00 (7.2301e+00)	Acc@1   0.00 (  0.08)	Acc@5   0.00 (  0.59)
Test: [ 44/196]	Time  0.069 ( 0.467)	Loss 7.6202e+00 (7.2388e+00)	Acc@1   0.00 (  0.08)	Acc@5   0.00 (  0.58)
Test: [ 45/196]	Time  0.069 ( 0.458)	Loss 7.7409e+00 (7.2497e+00)	Acc@1   0.00 (  0.08)	Acc@5   0.00 (  0.57)
Test: [ 46/196]	Time  0.069 ( 0.450)	Loss 7.7697e+00 (7.2608e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.56)
Test: [ 47/196]	Time  0.069 ( 0.442)	Loss 7.5826e+00 (7.2675e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.55)
Test: [ 48/196]	Time  0.069 ( 0.434)	Loss 7.1342e+00 (7.2648e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.53)
Test: [ 49/196]	Time  0.069 ( 0.427)	Loss 7.5750e+00 (7.2710e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.52)
Test: [ 50/196]	Time  0.074 ( 0.420)	Loss 7.6813e+00 (7.2790e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.51)
Test: [ 51/196]	Time  0.072 ( 0.413)	Loss 7.8563e+00 (7.2901e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.50)
Test: [ 52/196]	Time  0.070 ( 0.407)	Loss 7.0737e+00 (7.2860e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.49)
Test: [ 53/196]	Time  0.070 ( 0.401)	Loss 7.3014e+00 (7.2863e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.48)
Test: [ 54/196]	Time  0.069 ( 0.395)	Loss 6.8834e+00 (7.2790e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.48)
Test: [ 55/196]	Time  0.070 ( 0.389)	Loss 6.5537e+00 (7.2660e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.47)
Test: [ 56/196]	Time  0.069 ( 0.383)	Loss 6.6746e+00 (7.2557e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.46)
Test: [ 57/196]	Time  0.071 ( 0.378)	Loss 6.8982e+00 (7.2495e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.45)
Test: [ 58/196]	Time  0.071 ( 0.373)	Loss 7.2970e+00 (7.2503e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.44)
Test: [ 59/196]	Time  0.069 ( 0.368)	Loss 7.3825e+00 (7.2525e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.44)
Test: [ 60/196]	Time  0.070 ( 0.363)	Loss 6.9530e+00 (7.2476e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.43)
Test: [ 61/196]	Time  0.071 ( 0.358)	Loss 7.3516e+00 (7.2493e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.42)
Test: [ 62/196]	Time  0.069 ( 0.353)	Loss 7.2686e+00 (7.2496e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.42)
Test: [ 63/196]	Time  0.069 ( 0.349)	Loss 7.2432e+00 (7.2495e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.41)
Test: [ 64/196]	Time  0.070 ( 0.345)	Loss 7.5378e+00 (7.2539e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.40)
Test: [ 65/196]	Time  0.070 ( 0.341)	Loss 7.5000e+00 (7.2576e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.40)
Test: [ 66/196]	Time  0.071 ( 0.336)	Loss 7.1256e+00 (7.2557e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.39)
Test: [ 67/196]	Time  0.069 ( 0.333)	Loss 7.2429e+00 (7.2555e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.39 (  0.39)
Test: [ 68/196]	Time  0.069 ( 0.329)	Loss 7.4005e+00 (7.2576e+00)	Acc@1   0.39 (  0.06)	Acc@5   0.78 (  0.40)
Test: [ 69/196]	Time  0.069 ( 0.325)	Loss 6.6220e+00 (7.2485e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.39 (  0.40)
Test: [ 70/196]	Time  0.069 ( 0.321)	Loss 6.5111e+00 (7.2381e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.39)
Test: [ 71/196]	Time  0.070 ( 0.318)	Loss 6.2627e+00 (7.2246e+00)	Acc@1   0.39 (  0.06)	Acc@5   1.56 (  0.41)
Test: [ 72/196]	Time  0.073 ( 0.315)	Loss 6.6012e+00 (7.2160e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.40)
Test: [ 73/196]	Time  0.069 ( 0.311)	Loss 6.5566e+00 (7.2071e+00)	Acc@1   0.00 (  0.06)	Acc@5   1.17 (  0.41)
Test: [ 74/196]	Time  0.070 ( 0.308)	Loss 5.9148e+00 (7.1899e+00)	Acc@1   0.00 (  0.06)	Acc@5   1.17 (  0.42)
Test: [ 75/196]	Time  0.071 ( 0.305)	Loss 6.5495e+00 (7.1815e+00)	Acc@1   0.00 (  0.06)	Acc@5   3.91 (  0.47)
Test: [ 76/196]	Time  0.075 ( 0.302)	Loss 7.5147e+00 (7.1858e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.46)
Test: [ 77/196]	Time  0.070 ( 0.299)	Loss 7.2909e+00 (7.1871e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.46)
Test: [ 78/196]	Time  0.069 ( 0.296)	Loss 7.8343e+00 (7.1953e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.45)
Test: [ 79/196]	Time  0.075 ( 0.293)	Loss 7.7907e+00 (7.2028e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.44)
Test: [ 80/196]	Time  0.070 ( 0.291)	Loss 7.1636e+00 (7.2023e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.44)
Test: [ 81/196]	Time  0.070 ( 0.288)	Loss 7.5597e+00 (7.2066e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.43)
Test: [ 82/196]	Time  0.070 ( 0.285)	Loss 6.6769e+00 (7.2003e+00)	Acc@1   0.39 (  0.06)	Acc@5   5.08 (  0.49)
Test: [ 83/196]	Time  0.069 ( 0.283)	Loss 7.4248e+00 (7.2029e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.39 (  0.49)
Test: [ 84/196]	Time  0.070 ( 0.280)	Loss 7.7836e+00 (7.2098e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.48)
Test: [ 85/196]	Time  0.069 ( 0.278)	Loss 7.1053e+00 (7.2086e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.48)
Test: [ 86/196]	Time  0.070 ( 0.275)	Loss 6.6131e+00 (7.2017e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.47)
Test: [ 87/196]	Time  0.073 ( 0.273)	Loss 7.0329e+00 (7.1998e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.47)
Test: [ 88/196]	Time  0.070 ( 0.271)	Loss 7.0405e+00 (7.1980e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.46)
Test: [ 89/196]	Time  0.069 ( 0.268)	Loss 6.5862e+00 (7.1912e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.39 (  0.46)
Test: [ 90/196]	Time  0.070 ( 0.266)	Loss 6.9504e+00 (7.1886e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.46)
Test: [ 91/196]	Time  0.070 ( 0.264)	Loss 7.1756e+00 (7.1884e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.45)
Test: [ 92/196]	Time  0.069 ( 0.262)	Loss 7.0668e+00 (7.1871e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.45)
Test: [ 93/196]	Time  0.070 ( 0.260)	Loss 7.2147e+00 (7.1874e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.44)
Test: [ 94/196]	Time  0.069 ( 0.258)	Loss 8.0927e+00 (7.1969e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.44)
Test: [ 95/196]	Time  0.069 ( 0.256)	Loss 6.6914e+00 (7.1917e+00)	Acc@1   0.78 (  0.06)	Acc@5   9.38 (  0.53)
Test: [ 96/196]	Time  0.069 ( 0.254)	Loss 7.7364e+00 (7.1973e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.52)
Test: [ 97/196]	Time  0.069 ( 0.252)	Loss 7.2734e+00 (7.1981e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.00 (  0.52)
Test: [ 98/196]	Time  0.069 ( 0.250)	Loss 7.2962e+00 (7.1991e+00)	Acc@1   0.00 (  0.06)	Acc@5   0.39 (  0.52)
Test: [ 99/196]	Time  0.069 ( 0.249)	Loss 7.6957e+00 (7.2040e+00)	Acc@1   0.00 (  0.05)	Acc@5   0.00 (  0.51)
Test: [100/196]	Time  0.071 ( 0.247)	Loss 6.9350e+00 (7.2014e+00)	Acc@1   1.56 (  0.07)	Acc@5  15.23 (  0.66)
Test: [101/196]	Time  0.069 ( 0.245)	Loss 7.3988e+00 (7.2033e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.65)
Test: [102/196]	Time  0.069 ( 0.243)	Loss 7.3115e+00 (7.2043e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.64)
Test: [103/196]	Time  0.072 ( 0.242)	Loss 7.8379e+00 (7.2104e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.64)
Test: [104/196]	Time  0.072 ( 0.240)	Loss 7.9549e+00 (7.2175e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.63)
Test: [105/196]	Time  0.068 ( 0.238)	Loss 7.3683e+00 (7.2189e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.63)
Test: [106/196]	Time  0.069 ( 0.237)	Loss 8.0759e+00 (7.2270e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.62)
Test: [107/196]	Time  0.070 ( 0.235)	Loss 7.5921e+00 (7.2303e+00)	Acc@1   0.39 (  0.07)	Acc@5   0.39 (  0.62)
Test: [108/196]	Time  0.069 ( 0.234)	Loss 7.3122e+00 (7.2311e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.39 (  0.62)
Test: [109/196]	Time  0.069 ( 0.232)	Loss 7.0314e+00 (7.2293e+00)	Acc@1   0.78 (  0.07)	Acc@5   5.47 (  0.66)
Test: [110/196]	Time  0.070 ( 0.231)	Loss 8.3119e+00 (7.2390e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.65)
Test: [111/196]	Time  0.069 ( 0.229)	Loss 7.1970e+00 (7.2386e+00)	Acc@1   0.39 (  0.08)	Acc@5   0.39 (  0.65)
Test: [112/196]	Time  0.073 ( 0.228)	Loss 6.8666e+00 (7.2354e+00)	Acc@1   0.00 (  0.08)	Acc@5   0.00 (  0.65)
Test: [113/196]	Time  0.069 ( 0.227)	Loss 6.9484e+00 (7.2328e+00)	Acc@1   0.00 (  0.08)	Acc@5   0.00 (  0.64)
Test: [114/196]	Time  0.069 ( 0.225)	Loss 7.2645e+00 (7.2331e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.64)
Test: [115/196]	Time  0.069 ( 0.224)	Loss 7.6857e+00 (7.2370e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.63)
Test: [116/196]	Time  0.069 ( 0.223)	Loss 7.6604e+00 (7.2406e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.62)
Test: [117/196]	Time  0.069 ( 0.221)	Loss 7.1698e+00 (7.2400e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.62)
Test: [118/196]	Time  0.071 ( 0.220)	Loss 7.5518e+00 (7.2427e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.61)
Test: [119/196]	Time  0.069 ( 0.219)	Loss 7.1292e+00 (7.2417e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.61)
Test: [120/196]	Time  0.070 ( 0.218)	Loss 6.7580e+00 (7.2377e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.60)
Test: [121/196]	Time  0.070 ( 0.216)	Loss 7.2161e+00 (7.2375e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.60)
Test: [122/196]	Time  0.072 ( 0.215)	Loss 7.5026e+00 (7.2397e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.59)
Test: [123/196]	Time  0.069 ( 0.214)	Loss 7.4648e+00 (7.2415e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.59)
Test: [124/196]	Time  0.072 ( 0.213)	Loss 7.0640e+00 (7.2401e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.58)
Test: [125/196]	Time  0.069 ( 0.212)	Loss 7.4198e+00 (7.2415e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.58)
Test: [126/196]	Time  0.070 ( 0.211)	Loss 7.6712e+00 (7.2449e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.58)
Test: [127/196]	Time  0.069 ( 0.209)	Loss 6.9954e+00 (7.2429e+00)	Acc@1   0.39 (  0.07)	Acc@5   2.34 (  0.59)
Test: [128/196]	Time  0.069 ( 0.208)	Loss 7.1181e+00 (7.2420e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.58)
Test: [129/196]	Time  0.069 ( 0.207)	Loss 7.4407e+00 (7.2435e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.58)
Test: [130/196]	Time  0.068 ( 0.206)	Loss 7.4720e+00 (7.2452e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.58)
Test: [131/196]	Time  0.068 ( 0.205)	Loss 7.5973e+00 (7.2479e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.57)
Test: [132/196]	Time  0.068 ( 0.204)	Loss 7.0593e+00 (7.2465e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.57)
Test: [133/196]	Time  0.068 ( 0.203)	Loss 7.9241e+00 (7.2516e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.56)
Test: [134/196]	Time  0.068 ( 0.202)	Loss 8.0215e+00 (7.2573e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.56)
Test: [135/196]	Time  0.068 ( 0.201)	Loss 7.2703e+00 (7.2574e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.00 (  0.55)
Test: [136/196]	Time  0.068 ( 0.200)	Loss 6.7866e+00 (7.2539e+00)	Acc@1   0.00 (  0.07)	Acc@5   0.39 (  0.55)
Test: [137/196]	Time  0.068 ( 0.199)	Loss 7.1310e+00 (7.2530e+00)	Acc@1   1.56 (  0.08)	Acc@5   8.98 (  0.61)
Test: [138/196]	Time  0.068 ( 0.198)	Loss 7.4220e+00 (7.2542e+00)	Acc@1   0.00 (  0.08)	Acc@5   0.00 (  0.61)
Test: [139/196]	Time  0.069 ( 0.197)	Loss 7.4349e+00 (7.2555e+00)	Acc@1  10.94 (  0.15)	Acc@5  14.06 (  0.71)
Test: [140/196]	Time  0.068 ( 0.196)	Loss 7.4515e+00 (7.2569e+00)	Acc@1   2.73 (  0.17)	Acc@5   3.91 (  0.73)
Test: [141/196]	Time  0.068 ( 0.196)	Loss 7.3105e+00 (7.2573e+00)	Acc@1   0.00 (  0.17)	Acc@5   0.00 (  0.72)
Test: [142/196]	Time  0.068 ( 0.195)	Loss 7.6068e+00 (7.2597e+00)	Acc@1   0.00 (  0.17)	Acc@5   0.00 (  0.72)
Test: [143/196]	Time  0.068 ( 0.194)	Loss 7.6467e+00 (7.2624e+00)	Acc@1   0.00 (  0.17)	Acc@5   0.00 (  0.71)
Test: [144/196]	Time  0.068 ( 0.193)	Loss 7.2558e+00 (7.2624e+00)	Acc@1   0.00 (  0.17)	Acc@5   6.64 (  0.75)
Test: [145/196]	Time  0.068 ( 0.192)	Loss 7.7362e+00 (7.2656e+00)	Acc@1   0.00 (  0.17)	Acc@5   0.00 (  0.75)
Test: [146/196]	Time  0.068 ( 0.191)	Loss 7.4185e+00 (7.2667e+00)	Acc@1   0.00 (  0.16)	Acc@5   0.00 (  0.74)
Test: [147/196]	Time  0.068 ( 0.190)	Loss 7.5312e+00 (7.2685e+00)	Acc@1   0.00 (  0.16)	Acc@5   0.00 (  0.74)
Test: [148/196]	Time  0.068 ( 0.190)	Loss 7.3036e+00 (7.2687e+00)	Acc@1   0.00 (  0.16)	Acc@5   0.00 (  0.73)
Test: [149/196]	Time  0.068 ( 0.189)	Loss 7.3266e+00 (7.2691e+00)	Acc@1   0.00 (  0.16)	Acc@5   0.78 (  0.73)
Test: [150/196]	Time  0.072 ( 0.188)	Loss 7.0315e+00 (7.2675e+00)	Acc@1   0.00 (  0.16)	Acc@5   0.00 (  0.73)
Test: [151/196]	Time  0.068 ( 0.187)	Loss 6.9004e+00 (7.2651e+00)	Acc@1   0.00 (  0.16)	Acc@5   0.00 (  0.72)
Test: [152/196]	Time  0.068 ( 0.186)	Loss 7.2660e+00 (7.2651e+00)	Acc@1   0.00 (  0.16)	Acc@5   0.00 (  0.72)
Test: [153/196]	Time  0.068 ( 0.186)	Loss 7.2456e+00 (7.2650e+00)	Acc@1   0.00 (  0.16)	Acc@5   0.00 (  0.72)
Test: [154/196]	Time  0.068 ( 0.185)	Loss 7.5772e+00 (7.2670e+00)	Acc@1   0.00 (  0.16)	Acc@5   0.00 (  0.71)
Test: [155/196]	Time  0.068 ( 0.184)	Loss 7.9725e+00 (7.2715e+00)	Acc@1   0.00 (  0.16)	Acc@5   0.00 (  0.71)
Test: [156/196]	Time  0.068 ( 0.183)	Loss 8.3474e+00 (7.2784e+00)	Acc@1   0.00 (  0.15)	Acc@5   0.00 (  0.70)
Test: [157/196]	Time  0.068 ( 0.183)	Loss 7.2817e+00 (7.2784e+00)	Acc@1   0.00 (  0.15)	Acc@5   1.56 (  0.71)
Test: [158/196]	Time  0.070 ( 0.182)	Loss 8.0845e+00 (7.2835e+00)	Acc@1   0.00 (  0.15)	Acc@5   0.00 (  0.70)
Test: [159/196]	Time  0.068 ( 0.181)	Loss 7.4583e+00 (7.2845e+00)	Acc@1   0.00 (  0.15)	Acc@5   0.00 (  0.70)
Test: [160/196]	Time  0.068 ( 0.181)	Loss 7.3411e+00 (7.2849e+00)	Acc@1   0.00 (  0.15)	Acc@5   0.00 (  0.69)
Test: [161/196]	Time  0.068 ( 0.180)	Loss 7.6453e+00 (7.2871e+00)	Acc@1   0.00 (  0.15)	Acc@5   0.00 (  0.69)
Test: [162/196]	Time  0.068 ( 0.179)	Loss 7.7166e+00 (7.2898e+00)	Acc@1   0.00 (  0.15)	Acc@5   0.00 (  0.69)
Test: [163/196]	Time  0.075 ( 0.178)	Loss 6.3987e+00 (7.2843e+00)	Acc@1   0.00 (  0.15)	Acc@5   0.00 (  0.68)
Test: [164/196]	Time  0.068 ( 0.178)	Loss 7.1492e+00 (7.2835e+00)	Acc@1   0.00 (  0.15)	Acc@5   0.00 (  0.68)
Test: [165/196]	Time  0.068 ( 0.177)	Loss 7.3239e+00 (7.2837e+00)	Acc@1   0.00 (  0.15)	Acc@5   0.00 (  0.67)
Test: [166/196]	Time  0.068 ( 0.176)	Loss 7.3641e+00 (7.2842e+00)	Acc@1   0.00 (  0.15)	Acc@5   0.00 (  0.67)
Test: [167/196]	Time  0.068 ( 0.176)	Loss 7.3681e+00 (7.2847e+00)	Acc@1   0.00 (  0.14)	Acc@5   0.00 (  0.66)
Test: [168/196]	Time  0.068 ( 0.175)	Loss 6.7960e+00 (7.2818e+00)	Acc@1   0.00 (  0.14)	Acc@5   0.00 (  0.66)
Test: [169/196]	Time  0.071 ( 0.175)	Loss 6.9442e+00 (7.2798e+00)	Acc@1   0.00 (  0.14)	Acc@5   0.00 (  0.66)
Test: [170/196]	Time  0.069 ( 0.174)	Loss 7.9469e+00 (7.2837e+00)	Acc@1   0.00 (  0.14)	Acc@5   0.00 (  0.65)
Test: [171/196]	Time  0.068 ( 0.173)	Loss 7.1082e+00 (7.2827e+00)	Acc@1   0.00 (  0.14)	Acc@5   0.00 (  0.65)
Test: [172/196]	Time  0.068 ( 0.173)	Loss 7.6728e+00 (7.2850e+00)	Acc@1   0.00 (  0.14)	Acc@5   0.00 (  0.65)
Test: [173/196]	Time  0.068 ( 0.172)	Loss 7.3497e+00 (7.2854e+00)	Acc@1   0.00 (  0.14)	Acc@5   0.00 (  0.64)
Test: [174/196]	Time  0.069 ( 0.172)	Loss 8.1568e+00 (7.2903e+00)	Acc@1   0.00 (  0.14)	Acc@5   0.00 (  0.64)
Test: [175/196]	Time  0.068 ( 0.171)	Loss 7.6365e+00 (7.2923e+00)	Acc@1   0.00 (  0.14)	Acc@5   0.00 (  0.63)
Test: [176/196]	Time  0.068 ( 0.170)	Loss 6.5511e+00 (7.2881e+00)	Acc@1   0.00 (  0.14)	Acc@5   1.56 (  0.64)
Test: [177/196]	Time  0.068 ( 0.170)	Loss 7.2673e+00 (7.2880e+00)	Acc@1   0.00 (  0.14)	Acc@5   0.39 (  0.64)
Test: [178/196]	Time  0.068 ( 0.169)	Loss 7.0845e+00 (7.2869e+00)	Acc@1   0.00 (  0.14)	Acc@5   0.00 (  0.64)
Test: [179/196]	Time  0.068 ( 0.169)	Loss 6.6955e+00 (7.2836e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.63)
Test: [180/196]	Time  0.069 ( 0.168)	Loss 7.7534e+00 (7.2862e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.63)
Test: [181/196]	Time  0.068 ( 0.168)	Loss 8.4361e+00 (7.2925e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.62)
Test: [182/196]	Time  0.068 ( 0.167)	Loss 8.6108e+00 (7.2997e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.62)
Test: [183/196]	Time  0.068 ( 0.166)	Loss 8.5222e+00 (7.3063e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.62)
Test: [184/196]	Time  0.068 ( 0.166)	Loss 8.0168e+00 (7.3102e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.61)
Test: [185/196]	Time  0.069 ( 0.165)	Loss 7.9618e+00 (7.3137e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.61)
Test: [186/196]	Time  0.071 ( 0.165)	Loss 8.1273e+00 (7.3180e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.61)
Test: [187/196]	Time  0.068 ( 0.164)	Loss 7.8891e+00 (7.3211e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.60)
Test: [188/196]	Time  0.068 ( 0.164)	Loss 8.4730e+00 (7.3272e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.60)
Test: [189/196]	Time  0.069 ( 0.163)	Loss 7.2801e+00 (7.3269e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.60)
Test: [190/196]	Time  0.068 ( 0.163)	Loss 7.2549e+00 (7.3265e+00)	Acc@1   0.00 (  0.13)	Acc@5   8.98 (  0.64)
Test: [191/196]	Time  0.068 ( 0.162)	Loss 7.1808e+00 (7.3258e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.64)
Test: [192/196]	Time  0.068 ( 0.162)	Loss 7.5617e+00 (7.3270e+00)	Acc@1   0.00 (  0.13)	Acc@5   0.00 (  0.64)
Test: [193/196]	Time  0.068 ( 0.161)	Loss 7.6618e+00 (7.3287e+00)	Acc@1   0.00 (  0.12)	Acc@5   0.00 (  0.63)
Test: [194/196]	Time  0.068 ( 0.161)	Loss 7.7398e+00 (7.3308e+00)	Acc@1   0.00 (  0.12)	Acc@5   0.00 (  0.63)
Test: [195/196]	Time  0.151 ( 0.161)	Loss 7.0417e+00 (7.3304e+00)	Acc@1   0.00 (  0.12)	Acc@5   0.00 (  0.63)
 * acc@1 0.124 acc@5 0.628
valid_top1_acc:  tensor(0.1240, device='cuda:0')
10/16 03:12:14 PM args = Namespace(batch_size=256, data='/home/sliuau/ternary_vit/dataset/imagenet-1k/imagenet', epochs=128, label_smooth=0.1, learning_rate=0.00125, load_pretrained_weight_student=None, momentum=0.9, n_bit=2, quantize_downsample='0', save='./models', student='resnet18_lsq', teacher='resnet101', weight_decay=0.0, workers=40)
10/16 03:12:16 PM student:
10/16 03:12:16 PM ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=64)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=64)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=64)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=64)
      (bias32): LearnableBias()
    )
  )
  (layer2): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=64)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=128)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=128)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=128)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=128)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=128)
      (bias32): LearnableBias()
    )
  )
  (layer3): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=128)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=256)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=256)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=256)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=256)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=256)
      (bias32): LearnableBias()
    )
  )
  (layer4): Sequential(
    (0): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=256)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=512)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=512)
      (bias32): LearnableBias()
    )
    (1): BasicBlock_LSQ(
      (bias11): LearnableBias()
      (prelu1): PReLU(num_parameters=512)
      (bias12): LearnableBias()
      (quan1): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv1): HardQuantizeConv()
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias21): LearnableBias()
      (prelu2): PReLU(num_parameters=512)
      (bias22): LearnableBias()
      (quan2): LSQ(bit=2, norm=(False, 1e-05, 1.0), all_positive=True, symmetric=False, per_channel=True)
      (conv2): HardQuantizeConv()
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (bias31): LearnableBias()
      (prelu3): PReLU(num_parameters=512)
      (bias32): LearnableBias()
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
weight_param: conv1.weight
weight_param: layer1.0.conv1.weight
weight_param: layer1.0.conv2.weight
weight_param: layer1.1.conv1.weight
weight_param: layer1.1.conv2.weight
weight_param: layer2.0.conv1.weight
weight_param: layer2.0.conv2.weight
weight_param: layer2.0.downsample.0.weight
weight_param: layer2.1.conv1.weight
weight_param: layer2.1.conv2.weight
weight_param: layer3.0.conv1.weight
weight_param: layer3.0.conv2.weight
weight_param: layer3.0.downsample.0.weight
weight_param: layer3.1.conv1.weight
weight_param: layer3.1.conv2.weight
weight_param: layer4.0.conv1.weight
weight_param: layer4.0.conv2.weight
weight_param: layer4.0.downsample.0.weight
weight_param: layer4.1.conv1.weight
weight_param: layer4.1.conv2.weight
checkpoint_tar:  ./models/resnet18-f37072fd.pth
True
learning_rate: 0.001240234375
Epoch: [0][   0/5005]	Time 18.953 (18.953)	Data  8.492 ( 8.492)	Loss 7.1121e+00 (7.1121e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Epoch: [0][   1/5005]	Time  0.328 ( 9.641)	Data  0.000 ( 4.246)	Loss 7.0871e+00 (7.0996e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.39)
Epoch: [0][   2/5005]	Time  0.415 ( 6.565)	Data  0.000 ( 2.831)	Loss 7.1434e+00 (7.1142e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.39 (  0.39)
Epoch: [0][   3/5005]	Time  0.319 ( 5.004)	Data  0.000 ( 2.123)	Loss 7.4112e+00 (7.1885e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.29)
Epoch: [0][   4/5005]	Time  0.321 ( 4.067)	Data  0.000 ( 1.699)	Loss 7.2535e+00 (7.2015e+00)	Acc@1   1.17 (  0.23)	Acc@5   2.73 (  0.78)
Epoch: [0][   5/5005]	Time  0.318 ( 3.442)	Data  0.000 ( 1.416)	Loss 7.3175e+00 (7.2208e+00)	Acc@1   0.00 (  0.20)	Acc@5   0.39 (  0.72)
Epoch: [0][   6/5005]	Time  0.319 ( 2.996)	Data  0.000 ( 1.213)	Loss 7.4791e+00 (7.2577e+00)	Acc@1   0.00 (  0.17)	Acc@5   0.78 (  0.73)
Epoch: [0][   7/5005]	Time  0.320 ( 2.662)	Data  0.000 ( 1.062)	Loss 7.4415e+00 (7.2807e+00)	Acc@1   0.39 (  0.20)	Acc@5   1.17 (  0.78)
Epoch: [0][   8/5005]	Time  0.319 ( 2.401)	Data  0.000 ( 0.944)	Loss 7.2164e+00 (7.2735e+00)	Acc@1   0.39 (  0.22)	Acc@5   1.17 (  0.82)
Epoch: [0][   9/5005]	Time  0.320 ( 2.193)	Data  0.000 ( 0.849)	Loss 7.2974e+00 (7.2759e+00)	Acc@1   0.39 (  0.23)	Acc@5   0.39 (  0.78)
Epoch: [0][  10/5005]	Time  0.319 ( 2.023)	Data  0.000 ( 0.772)	Loss 7.4470e+00 (7.2915e+00)	Acc@1   0.39 (  0.25)	Acc@5   1.17 (  0.82)
Epoch: [0][  11/5005]	Time  0.335 ( 1.882)	Data  0.000 ( 0.708)	Loss 7.1389e+00 (7.2788e+00)	Acc@1   0.78 (  0.29)	Acc@5   2.34 (  0.94)
Epoch: [0][  12/5005]	Time  0.318 ( 1.762)	Data  0.000 ( 0.654)	Loss 7.3283e+00 (7.2826e+00)	Acc@1   0.39 (  0.30)	Acc@5   2.34 (  1.05)
Epoch: [0][  13/5005]	Time  0.318 ( 1.659)	Data  0.000 ( 0.607)	Loss 7.1591e+00 (7.2738e+00)	Acc@1   1.17 (  0.36)	Acc@5   1.56 (  1.09)
Epoch: [0][  14/5005]	Time  0.319 ( 1.569)	Data  0.000 ( 0.566)	Loss 7.0494e+00 (7.2588e+00)	Acc@1   0.39 (  0.36)	Acc@5   1.95 (  1.15)
Epoch: [0][  15/5005]	Time  0.320 ( 1.491)	Data  0.000 ( 0.531)	Loss 6.9402e+00 (7.2389e+00)	Acc@1   0.39 (  0.37)	Acc@5   1.56 (  1.17)
Epoch: [0][  16/5005]	Time  0.318 ( 1.422)	Data  0.000 ( 0.500)	Loss 6.9791e+00 (7.2236e+00)	Acc@1   0.78 (  0.39)	Acc@5   1.17 (  1.17)
Epoch: [0][  17/5005]	Time  0.375 ( 1.364)	Data  0.000 ( 0.472)	Loss 6.9069e+00 (7.2060e+00)	Acc@1   0.39 (  0.39)	Acc@5   1.95 (  1.22)
Epoch: [0][  18/5005]	Time  0.319 ( 1.309)	Data  0.000 ( 0.447)	Loss 6.8564e+00 (7.1876e+00)	Acc@1   0.00 (  0.37)	Acc@5   0.78 (  1.19)
Epoch: [0][  19/5005]	Time  0.320 ( 1.260)	Data  0.000 ( 0.425)	Loss 6.8392e+00 (7.1702e+00)	Acc@1   0.39 (  0.37)	Acc@5   0.78 (  1.17)
Epoch: [0][  20/5005]	Time  0.325 ( 1.215)	Data  0.000 ( 0.405)	Loss 6.9255e+00 (7.1585e+00)	Acc@1   0.39 (  0.37)	Acc@5   0.78 (  1.15)
Epoch: [0][  21/5005]	Time  0.320 ( 1.174)	Data  0.000 ( 0.386)	Loss 6.7887e+00 (7.1417e+00)	Acc@1   0.39 (  0.37)	Acc@5   1.17 (  1.15)
Epoch: [0][  22/5005]	Time  0.320 ( 1.137)	Data  0.000 ( 0.369)	Loss 6.8359e+00 (7.1284e+00)	Acc@1   0.39 (  0.37)	Acc@5   2.73 (  1.22)
Epoch: [0][  23/5005]	Time  0.319 ( 1.103)	Data  0.000 ( 0.354)	Loss 6.8150e+00 (7.1154e+00)	Acc@1   0.78 (  0.39)	Acc@5   3.52 (  1.32)
Epoch: [0][  24/5005]	Time  0.320 ( 1.072)	Data  0.000 ( 0.340)	Loss 6.9098e+00 (7.1072e+00)	Acc@1   0.39 (  0.39)	Acc@5   1.56 (  1.33)
Epoch: [0][  25/5005]	Time  0.319 ( 1.043)	Data  0.000 ( 0.327)	Loss 6.7721e+00 (7.0943e+00)	Acc@1   0.78 (  0.41)	Acc@5   2.34 (  1.37)
Epoch: [0][  26/5005]	Time  0.318 ( 1.016)	Data  0.000 ( 0.315)	Loss 6.8290e+00 (7.0844e+00)	Acc@1   0.78 (  0.42)	Acc@5   1.17 (  1.36)
Epoch: [0][  27/5005]	Time  0.318 ( 0.991)	Data  0.000 ( 0.304)	Loss 6.6838e+00 (7.0701e+00)	Acc@1   1.56 (  0.46)	Acc@5   3.91 (  1.45)
Epoch: [0][  28/5005]	Time  0.319 ( 0.968)	Data  0.000 ( 0.293)	Loss 6.6806e+00 (7.0567e+00)	Acc@1   1.56 (  0.50)	Acc@5   3.91 (  1.54)
Epoch: [0][  29/5005]	Time  0.318 ( 0.946)	Data  0.000 ( 0.283)	Loss 6.7577e+00 (7.0467e+00)	Acc@1   0.78 (  0.51)	Acc@5   4.69 (  1.64)
Epoch: [0][  30/5005]	Time  0.323 ( 0.926)	Data  0.000 ( 0.274)	Loss 6.6465e+00 (7.0338e+00)	Acc@1   0.39 (  0.50)	Acc@5   2.73 (  1.68)
Epoch: [0][  31/5005]	Time  0.317 ( 0.907)	Data  0.000 ( 0.266)	Loss 6.5256e+00 (7.0179e+00)	Acc@1   0.78 (  0.51)	Acc@5   3.91 (  1.75)
Epoch: [0][  32/5005]	Time  0.319 ( 0.889)	Data  0.000 ( 0.258)	Loss 6.6451e+00 (7.0066e+00)	Acc@1   0.00 (  0.50)	Acc@5   3.12 (  1.79)
Epoch: [0][  33/5005]	Time  0.319 ( 0.872)	Data  0.000 ( 0.250)	Loss 6.5263e+00 (6.9925e+00)	Acc@1   1.95 (  0.54)	Acc@5   5.47 (  1.90)
Epoch: [0][  34/5005]	Time  0.319 ( 0.857)	Data  0.000 ( 0.243)	Loss 6.5939e+00 (6.9811e+00)	Acc@1   1.17 (  0.56)	Acc@5   6.25 (  2.02)
Epoch: [0][  35/5005]	Time  0.319 ( 0.842)	Data  0.000 ( 0.236)	Loss 6.5164e+00 (6.9682e+00)	Acc@1   0.39 (  0.55)	Acc@5   5.47 (  2.12)
Epoch: [0][  36/5005]	Time  0.320 ( 0.828)	Data  0.000 ( 0.230)	Loss 6.5985e+00 (6.9582e+00)	Acc@1   0.39 (  0.55)	Acc@5   3.52 (  2.15)
Epoch: [0][  37/5005]	Time  0.323 ( 0.814)	Data  0.000 ( 0.224)	Loss 6.6755e+00 (6.9508e+00)	Acc@1   0.00 (  0.53)	Acc@5   2.34 (  2.16)
Epoch: [0][  38/5005]	Time  0.318 ( 0.802)	Data  0.000 ( 0.218)	Loss 6.6113e+00 (6.9421e+00)	Acc@1   1.56 (  0.56)	Acc@5   2.34 (  2.16)
Epoch: [0][  39/5005]	Time  0.318 ( 0.790)	Data  0.000 ( 0.213)	Loss 6.6977e+00 (6.9360e+00)	Acc@1   1.17 (  0.58)	Acc@5   4.30 (  2.22)
Epoch: [0][  40/5005]	Time  0.318 ( 0.778)	Data  0.000 ( 0.207)	Loss 6.5850e+00 (6.9274e+00)	Acc@1   1.17 (  0.59)	Acc@5   3.12 (  2.24)
Epoch: [0][  41/5005]	Time  0.318 ( 0.767)	Data  0.000 ( 0.202)	Loss 6.4044e+00 (6.9150e+00)	Acc@1   1.17 (  0.60)	Acc@5   5.08 (  2.31)
Epoch: [0][  42/5005]	Time  0.405 ( 0.759)	Data  0.000 ( 0.198)	Loss 6.4939e+00 (6.9052e+00)	Acc@1   1.17 (  0.62)	Acc@5   5.08 (  2.37)
Epoch: [0][  43/5005]	Time  0.318 ( 0.749)	Data  0.000 ( 0.193)	Loss 6.4906e+00 (6.8957e+00)	Acc@1   1.56 (  0.64)	Acc@5   5.08 (  2.43)
Epoch: [0][  44/5005]	Time  0.318 ( 0.739)	Data  0.000 ( 0.189)	Loss 6.4493e+00 (6.8858e+00)	Acc@1   0.39 (  0.63)	Acc@5   4.69 (  2.48)
Epoch: [0][  45/5005]	Time  0.319 ( 0.730)	Data  0.000 ( 0.185)	Loss 6.4038e+00 (6.8753e+00)	Acc@1   1.95 (  0.66)	Acc@5   6.25 (  2.56)
Epoch: [0][  46/5005]	Time  0.318 ( 0.721)	Data  0.000 ( 0.181)	Loss 6.3889e+00 (6.8650e+00)	Acc@1   0.00 (  0.65)	Acc@5   3.12 (  2.58)
Epoch: [0][  47/5005]	Time  0.321 ( 0.713)	Data  0.000 ( 0.177)	Loss 6.4723e+00 (6.8568e+00)	Acc@1   0.39 (  0.64)	Acc@5   3.52 (  2.60)
Epoch: [0][  48/5005]	Time  0.319 ( 0.705)	Data  0.000 ( 0.174)	Loss 6.4364e+00 (6.8482e+00)	Acc@1   0.78 (  0.65)	Acc@5   5.86 (  2.66)
Epoch: [0][  49/5005]	Time  0.320 ( 0.697)	Data  0.000 ( 0.170)	Loss 6.4142e+00 (6.8395e+00)	Acc@1   1.17 (  0.66)	Acc@5   5.47 (  2.72)
Epoch: [0][  50/5005]	Time  0.318 ( 0.690)	Data  0.000 ( 0.167)	Loss 6.3713e+00 (6.8304e+00)	Acc@1   1.17 (  0.67)	Acc@5   2.73 (  2.72)
Epoch: [0][  51/5005]	Time  0.319 ( 0.683)	Data  0.000 ( 0.164)	Loss 6.5742e+00 (6.8254e+00)	Acc@1   0.00 (  0.65)	Acc@5   2.73 (  2.72)
Epoch: [0][  52/5005]	Time  0.318 ( 0.676)	Data  0.000 ( 0.160)	Loss 6.4353e+00 (6.8181e+00)	Acc@1   1.17 (  0.66)	Acc@5   4.69 (  2.76)
Epoch: [0][  53/5005]	Time  0.323 ( 0.669)	Data  0.000 ( 0.158)	Loss 6.4359e+00 (6.8110e+00)	Acc@1   0.00 (  0.65)	Acc@5   5.86 (  2.81)
Epoch: [0][  54/5005]	Time  0.319 ( 0.663)	Data  0.001 ( 0.155)	Loss 6.4129e+00 (6.8038e+00)	Acc@1   0.78 (  0.65)	Acc@5   3.52 (  2.83)
Epoch: [0][  55/5005]	Time  0.320 ( 0.657)	Data  0.000 ( 0.152)	Loss 6.3829e+00 (6.7962e+00)	Acc@1   1.95 (  0.68)	Acc@5   3.91 (  2.85)
Epoch: [0][  56/5005]	Time  0.320 ( 0.651)	Data  0.000 ( 0.149)	Loss 6.4683e+00 (6.7905e+00)	Acc@1   1.17 (  0.69)	Acc@5   2.34 (  2.84)
Epoch: [0][  57/5005]	Time  0.319 ( 0.645)	Data  0.001 ( 0.147)	Loss 6.3387e+00 (6.7827e+00)	Acc@1   2.34 (  0.71)	Acc@5   5.86 (  2.89)
Epoch: [0][  58/5005]	Time  0.325 ( 0.640)	Data  0.000 ( 0.144)	Loss 6.2591e+00 (6.7738e+00)	Acc@1   1.17 (  0.72)	Acc@5   5.47 (  2.93)
Epoch: [0][  59/5005]	Time  0.394 ( 0.636)	Data  0.000 ( 0.142)	Loss 6.3249e+00 (6.7663e+00)	Acc@1   0.00 (  0.71)	Acc@5   3.91 (  2.95)
Epoch: [0][  60/5005]	Time  0.319 ( 0.630)	Data  0.000 ( 0.139)	Loss 6.4678e+00 (6.7615e+00)	Acc@1   0.39 (  0.70)	Acc@5   3.12 (  2.95)
Epoch: [0][  61/5005]	Time  0.319 ( 0.625)	Data  0.000 ( 0.137)	Loss 6.3946e+00 (6.7555e+00)	Acc@1   0.39 (  0.70)	Acc@5   3.91 (  2.97)
Epoch: [0][  62/5005]	Time  0.320 ( 0.620)	Data  0.000 ( 0.135)	Loss 6.3890e+00 (6.7497e+00)	Acc@1   0.39 (  0.69)	Acc@5   5.86 (  3.01)
Epoch: [0][  63/5005]	Time  0.320 ( 0.616)	Data  0.000 ( 0.133)	Loss 6.3488e+00 (6.7435e+00)	Acc@1   0.39 (  0.69)	Acc@5   2.34 (  3.00)
Epoch: [0][  64/5005]	Time  0.320 ( 0.611)	Data  0.000 ( 0.131)	Loss 6.2861e+00 (6.7364e+00)	Acc@1   1.56 (  0.70)	Acc@5   5.47 (  3.04)
Epoch: [0][  65/5005]	Time  0.321 ( 0.607)	Data  0.000 ( 0.129)	Loss 6.3747e+00 (6.7309e+00)	Acc@1   0.39 (  0.70)	Acc@5   5.08 (  3.07)
Epoch: [0][  66/5005]	Time  0.320 ( 0.603)	Data  0.000 ( 0.127)	Loss 6.4318e+00 (6.7265e+00)	Acc@1   0.39 (  0.69)	Acc@5   3.12 (  3.07)
Epoch: [0][  67/5005]	Time  0.320 ( 0.598)	Data  0.000 ( 0.125)	Loss 6.3026e+00 (6.7202e+00)	Acc@1   2.34 (  0.72)	Acc@5   6.64 (  3.12)
Epoch: [0][  68/5005]	Time  0.319 ( 0.594)	Data  0.000 ( 0.123)	Loss 6.3702e+00 (6.7152e+00)	Acc@1   1.56 (  0.73)	Acc@5   5.08 (  3.15)
Epoch: [0][  69/5005]	Time  0.320 ( 0.590)	Data  0.000 ( 0.122)	Loss 6.4019e+00 (6.7107e+00)	Acc@1   0.39 (  0.73)	Acc@5   3.91 (  3.16)
Epoch: [0][  70/5005]	Time  0.321 ( 0.587)	Data  0.000 ( 0.120)	Loss 6.1355e+00 (6.7026e+00)	Acc@1   1.17 (  0.73)	Acc@5   4.30 (  3.18)
Epoch: [0][  71/5005]	Time  0.319 ( 0.583)	Data  0.000 ( 0.118)	Loss 6.2784e+00 (6.6967e+00)	Acc@1   1.17 (  0.74)	Acc@5   5.08 (  3.21)
Epoch: [0][  72/5005]	Time  0.324 ( 0.579)	Data  0.000 ( 0.117)	Loss 6.3091e+00 (6.6914e+00)	Acc@1   1.56 (  0.75)	Acc@5   5.86 (  3.24)
Epoch: [0][  73/5005]	Time  0.319 ( 0.576)	Data  0.000 ( 0.115)	Loss 6.1974e+00 (6.6847e+00)	Acc@1   2.34 (  0.77)	Acc@5   5.47 (  3.27)
Epoch: [0][  74/5005]	Time  0.381 ( 0.573)	Data  0.000 ( 0.113)	Loss 6.3913e+00 (6.6808e+00)	Acc@1   0.39 (  0.77)	Acc@5   5.47 (  3.30)
Epoch: [0][  75/5005]	Time  0.322 ( 0.570)	Data  0.000 ( 0.112)	Loss 6.1604e+00 (6.6740e+00)	Acc@1   1.56 (  0.78)	Acc@5   8.20 (  3.37)
Epoch: [0][  76/5005]	Time  0.325 ( 0.567)	Data  0.000 ( 0.111)	Loss 6.1109e+00 (6.6666e+00)	Acc@1   1.95 (  0.79)	Acc@5   6.25 (  3.40)
Epoch: [0][  77/5005]	Time  0.319 ( 0.564)	Data  0.000 ( 0.109)	Loss 6.2416e+00 (6.6612e+00)	Acc@1   1.56 (  0.80)	Acc@5   5.86 (  3.44)
Epoch: [0][  78/5005]	Time  0.320 ( 0.560)	Data  0.000 ( 0.108)	Loss 6.3583e+00 (6.6574e+00)	Acc@1   0.39 (  0.80)	Acc@5   4.69 (  3.45)
Epoch: [0][  79/5005]	Time  0.320 ( 0.557)	Data  0.000 ( 0.106)	Loss 6.2740e+00 (6.6526e+00)	Acc@1   0.39 (  0.79)	Acc@5   4.30 (  3.46)
Epoch: [0][  80/5005]	Time  0.319 ( 0.555)	Data  0.000 ( 0.105)	Loss 6.1813e+00 (6.6467e+00)	Acc@1   2.73 (  0.82)	Acc@5   6.64 (  3.50)
Epoch: [0][  81/5005]	Time  0.319 ( 0.552)	Data  0.001 ( 0.104)	Loss 6.3137e+00 (6.6427e+00)	Acc@1   1.95 (  0.83)	Acc@5   5.86 (  3.53)
Epoch: [0][  82/5005]	Time  0.319 ( 0.549)	Data  0.000 ( 0.103)	Loss 6.2065e+00 (6.6374e+00)	Acc@1   0.39 (  0.82)	Acc@5   4.30 (  3.54)
Epoch: [0][  83/5005]	Time  0.320 ( 0.546)	Data  0.000 ( 0.101)	Loss 6.2173e+00 (6.6324e+00)	Acc@1   0.78 (  0.82)	Acc@5   5.86 (  3.57)
Epoch: [0][  84/5005]	Time  0.319 ( 0.543)	Data  0.000 ( 0.100)	Loss 6.1223e+00 (6.6264e+00)	Acc@1   1.56 (  0.83)	Acc@5   7.03 (  3.61)
Epoch: [0][  85/5005]	Time  0.319 ( 0.541)	Data  0.000 ( 0.099)	Loss 6.2286e+00 (6.6218e+00)	Acc@1   1.56 (  0.84)	Acc@5   5.86 (  3.63)
Epoch: [0][  86/5005]	Time  0.319 ( 0.538)	Data  0.000 ( 0.098)	Loss 6.0836e+00 (6.6156e+00)	Acc@1   1.17 (  0.84)	Acc@5   7.03 (  3.67)
Epoch: [0][  87/5005]	Time  0.319 ( 0.536)	Data  0.000 ( 0.097)	Loss 6.1416e+00 (6.6102e+00)	Acc@1   1.56 (  0.85)	Acc@5   3.91 (  3.68)
Epoch: [0][  88/5005]	Time  0.326 ( 0.533)	Data  0.000 ( 0.096)	Loss 6.0658e+00 (6.6041e+00)	Acc@1   1.95 (  0.86)	Acc@5   7.42 (  3.72)
Epoch: [0][  89/5005]	Time  0.320 ( 0.531)	Data  0.000 ( 0.095)	Loss 6.0345e+00 (6.5978e+00)	Acc@1   1.95 (  0.88)	Acc@5   5.08 (  3.73)
Epoch: [0][  90/5005]	Time  0.322 ( 0.529)	Data  0.000 ( 0.094)	Loss 6.0439e+00 (6.5917e+00)	Acc@1   1.56 (  0.88)	Acc@5   7.42 (  3.77)
Epoch: [0][  91/5005]	Time  0.319 ( 0.527)	Data  0.000 ( 0.093)	Loss 6.0884e+00 (6.5862e+00)	Acc@1   1.56 (  0.89)	Acc@5   7.81 (  3.82)
Epoch: [0][  92/5005]	Time  0.320 ( 0.524)	Data  0.000 ( 0.092)	Loss 6.1870e+00 (6.5819e+00)	Acc@1   1.95 (  0.90)	Acc@5   7.81 (  3.86)
Epoch: [0][  93/5005]	Time  0.319 ( 0.522)	Data  0.000 ( 0.091)	Loss 6.1934e+00 (6.5778e+00)	Acc@1   1.95 (  0.91)	Acc@5   7.03 (  3.89)
Epoch: [0][  94/5005]	Time  0.319 ( 0.520)	Data  0.000 ( 0.090)	Loss 6.2955e+00 (6.5748e+00)	Acc@1   0.78 (  0.91)	Acc@5   3.12 (  3.89)
Epoch: [0][  95/5005]	Time  0.319 ( 0.518)	Data  0.000 ( 0.089)	Loss 6.0970e+00 (6.5699e+00)	Acc@1   1.17 (  0.92)	Acc@5   6.64 (  3.91)
Epoch: [0][  96/5005]	Time  0.323 ( 0.516)	Data  0.000 ( 0.088)	Loss 6.2388e+00 (6.5664e+00)	Acc@1   1.56 (  0.92)	Acc@5   7.03 (  3.95)
Epoch: [0][  97/5005]	Time  0.320 ( 0.514)	Data  0.000 ( 0.087)	Loss 6.0318e+00 (6.5610e+00)	Acc@1   2.34 (  0.94)	Acc@5   7.42 (  3.98)
Epoch: [0][  98/5005]	Time  0.319 ( 0.512)	Data  0.000 ( 0.086)	Loss 6.0542e+00 (6.5559e+00)	Acc@1   1.95 (  0.95)	Acc@5   8.20 (  4.02)
Epoch: [0][  99/5005]	Time  0.319 ( 0.510)	Data  0.000 ( 0.085)	Loss 6.0457e+00 (6.5508e+00)	Acc@1   0.78 (  0.95)	Acc@5   5.86 (  4.04)
Epoch: [0][ 100/5005]	Time  0.388 ( 0.509)	Data  0.000 ( 0.084)	Loss 6.0380e+00 (6.5457e+00)	Acc@1   1.95 (  0.96)	Acc@5   7.03 (  4.07)
Epoch: [0][ 101/5005]	Time  0.320 ( 0.507)	Data  0.000 ( 0.084)	Loss 6.1608e+00 (6.5419e+00)	Acc@1   0.00 (  0.95)	Acc@5   3.91 (  4.07)
Epoch: [0][ 102/5005]	Time  0.319 ( 0.505)	Data  0.000 ( 0.083)	Loss 6.0227e+00 (6.5369e+00)	Acc@1   1.95 (  0.96)	Acc@5   5.08 (  4.08)
Epoch: [0][ 103/5005]	Time  0.319 ( 0.503)	Data  0.000 ( 0.082)	Loss 6.0960e+00 (6.5326e+00)	Acc@1   3.91 (  0.98)	Acc@5  10.55 (  4.14)
Epoch: [0][ 104/5005]	Time  0.319 ( 0.502)	Data  0.000 ( 0.081)	Loss 6.1512e+00 (6.5290e+00)	Acc@1   2.73 (  1.00)	Acc@5   5.47 (  4.16)
Epoch: [0][ 105/5005]	Time  0.319 ( 0.500)	Data  0.000 ( 0.080)	Loss 6.0742e+00 (6.5247e+00)	Acc@1   1.17 (  1.00)	Acc@5   4.69 (  4.16)
Epoch: [0][ 106/5005]	Time  0.319 ( 0.498)	Data  0.000 ( 0.080)	Loss 6.1903e+00 (6.5216e+00)	Acc@1   1.95 (  1.01)	Acc@5   7.42 (  4.19)
Epoch: [0][ 107/5005]	Time  0.319 ( 0.496)	Data  0.000 ( 0.079)	Loss 6.0679e+00 (6.5174e+00)	Acc@1   3.12 (  1.03)	Acc@5   9.38 (  4.24)
Epoch: [0][ 108/5005]	Time  0.319 ( 0.495)	Data  0.000 ( 0.078)	Loss 5.9726e+00 (6.5124e+00)	Acc@1   1.17 (  1.03)	Acc@5   5.47 (  4.25)
Epoch: [0][ 109/5005]	Time  0.320 ( 0.493)	Data  0.000 ( 0.077)	Loss 6.0671e+00 (6.5083e+00)	Acc@1   3.91 (  1.06)	Acc@5   9.38 (  4.30)
Epoch: [0][ 110/5005]	Time  0.319 ( 0.492)	Data  0.000 ( 0.077)	Loss 6.0309e+00 (6.5040e+00)	Acc@1   2.73 (  1.07)	Acc@5   8.98 (  4.34)
Epoch: [0][ 111/5005]	Time  0.320 ( 0.490)	Data  0.000 ( 0.076)	Loss 6.2376e+00 (6.5017e+00)	Acc@1   0.78 (  1.07)	Acc@5   4.69 (  4.34)
Epoch: [0][ 112/5005]	Time  0.319 ( 0.489)	Data  0.000 ( 0.075)	Loss 6.0015e+00 (6.4972e+00)	Acc@1   1.95 (  1.08)	Acc@5   8.98 (  4.38)
Epoch: [0][ 113/5005]	Time  0.325 ( 0.487)	Data  0.000 ( 0.075)	Loss 6.0261e+00 (6.4931e+00)	Acc@1   0.78 (  1.08)	Acc@5   8.20 (  4.42)
Epoch: [0][ 114/5005]	Time  0.318 ( 0.486)	Data  0.000 ( 0.074)	Loss 6.0525e+00 (6.4893e+00)	Acc@1   1.56 (  1.08)	Acc@5   5.86 (  4.43)
Epoch: [0][ 115/5005]	Time  0.319 ( 0.484)	Data  0.000 ( 0.073)	Loss 5.9495e+00 (6.4846e+00)	Acc@1   2.73 (  1.09)	Acc@5   7.81 (  4.46)
Epoch: [0][ 116/5005]	Time  0.320 ( 0.483)	Data  0.000 ( 0.073)	Loss 6.0926e+00 (6.4813e+00)	Acc@1   3.12 (  1.11)	Acc@5   7.81 (  4.49)
Epoch: [0][ 117/5005]	Time  0.319 ( 0.482)	Data  0.000 ( 0.072)	Loss 5.8909e+00 (6.4763e+00)	Acc@1   3.52 (  1.13)	Acc@5   9.77 (  4.53)
